import os
import logging
from dotenv import load_dotenv
from langchain_cohere import ChatCohere
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from src.build_vectorstore import get_vectorstore

# Load environment variables
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def query_rag(question, soil_type="Non sp√©cifi√©", location="S√©n√©gal", chat_history=None):
    """
    Ex√©cute une requ√™te via la cha√Æne RAG avec agent de reformulation pour les follow-ups.
    G√©n√©rateur qui yield des √©v√©nements de type:
    - {"type": "status", "content": "Message de statut..."}
    - {"type": "chunk", "content": "Texte partiel de la r√©ponse..."}
    """
    if chat_history is None:
        chat_history = []
        
    # --- PHASE 0 : V√âRIFICATIONS ---
    yield {"type": "status", "content": "V√©rification de la base de connaissances..."}
    vectorstore = get_vectorstore()
    if not vectorstore:
        yield {"type": "chunk", "content": "D√©sol√©, la base de connaissances n'est pas disponible actuellement."}
        return
        
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    llm = ChatCohere(model="command-r-08-2024")
    
    def format_history(history):
        formatted = ""
        for user_msg, ai_msg in history:
            formatted += f"Utilisateur: {user_msg}\nAssistant: {ai_msg}\n"
        return formatted

    # --- √âTAPE 1 : CONTEXTUALISATION ---
    # Cette √©tape transforme une question de suivi (ex: "Et pour l'engrais ?") 
    # en une question autonome compr√©hensible par le moteur de recherche.
    
    contextualize_template = """
    √âtant donn√© l'historique de la conversation et la question actuelle de l'utilisateur, 
    si la question fait r√©f√©rence √† des √©l√©ments pr√©c√©dents, reformulez-la en une question autonome 
    qui peut √™tre comprise sans l'historique. Ne r√©pondez pas √† la question, reformulez-la simplement.
    Si la question est d√©j√† autonome, renvoyez-la telle quelle.
    
    HISTORIQUE :
    {chat_history}
    
    QUESTION ACTUELLE : {question}
    
    QUESTION AUTONOME REFORMUL√âE :
    """
    contextualize_prompt = ChatPromptTemplate.from_template(contextualize_template)
    contextualize_chain = contextualize_prompt | llm | StrOutputParser()
    
    standalone_question = question
    if chat_history:
        yield {"type": "status", "content": "Compr√©hension du contexte..."}
        standalone_question = contextualize_chain.invoke({
            "chat_history": format_history(chat_history),
            "question": question
        })
        logger.info(f"Question reformul√©e : {standalone_question}")

    # --- √âTAPE 2 : R√âPONSE FINALE AVEC RAG ---
    yield {"type": "status", "content": "Recherche d'informations pertinentes..."}
    
    # Prompt syst√®me ultra-structur√© 
    template = """
    # üéØ IDENTIT√â ET MANDAT
    Vous √™tes **T√®wou Agro-Assistant**, un expert agricole s√©n√©galais virtuel. Votre mission est d'accompagner les agriculteurs avec des conseils pratiques, pr√©cis et bienveillants, exclusivement centr√©s sur l'agriculture au S√©n√©gal.

    # üìú R√àGLES DE FONCTIONNEMENT
    ## DOMAINE D'EXPERTISE (NON-N√âGOCIABLE)
    - ‚úÖ **SUJETS AUTORIS√âS** : Agriculture s√©n√©galaise, cultures locales, sols, climat, m√©t√©o, saisons, irrigation, fertilisation, protection des cultures, calendriers culturaux
    - ‚ùå **SUJETS REFUS√âS** : Toute question hors agriculture s√©n√©galaise, politique, √©conomie g√©n√©rale, sant√© humaine, technologie hors agriculture
    - **R√àGLE D'OR** : Si une question sort de votre domaine, r√©pondez chaleureusement mais fermement : *"Je suis d√©sol√©, je suis sp√©cialis√© uniquement dans l'agriculture s√©n√©galaise. Je peux vous aider avec des questions sur les cultures, le sol, la m√©t√©o ou les pratiques agricoles au S√©n√©gal."*

    ## QUALIT√âS REQUISES
    - **Praticit√©** : Toujours donner des conseils applicables imm√©diatement
    - **Pr√©cision** : Utiliser les donn√©es contextuelles (sol, localisation)
    - **Empathie** : Comprendre les difficult√©s des agriculteurs
    - **Clart√©** : Expliquer les termes techniques simplement

    # üìä CONTEXTE UTILISATEUR (PERSONNALISATION)
    **Profil agricole :**
    - üå± **Type de sol** : {soil_type}
    - üìç **Localisation** : {location}

    # üìö BASE DE CONNAISSANCES (CONTEXTE R√âCUP√âR√â)
    {context}

    # üí¨ HISTORIQUE DE CONVERSATION (POUR R√âF√âRENCE)
    {chat_history}

    # üé§ QUESTION (CONSOLID√âE)
    {question}

    # ‚ú® INSTRUCTIONS DE R√âPONSE
    0. **{introduction_instruction}**
    1. **Accueil chaleureux** (Rapide si ce n'est pas le d√©but)
    2. **R√©ponse structur√©e** bas√©e sur le contexte et votre expertise
    3. **Application locale** li√©e √† {soil_type} et {location}
    4. **Citation des sources** (ex: "Selon les donn√©es m√©t√©o...")

    **Commencez maintenant votre r√©ponse :
    """

    prompt = ChatPromptTemplate.from_template(template)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # D√©termination de l'instruction de pr√©sentation
    intro_text = "Pr√©sentez-vous bri√®vement comme T√®wou Agro-Assistant." if not chat_history else "NE VOUS PR√âSENTEZ PAS. R√©pondez directement √† la question."

    # Pour le streaming, on doit construire la cha√Æne l√©g√®rement diff√©remment pour r√©cup√©rer les documents si besoin,
    # mais pour simplifier ici on garde la structure et on stream la r√©ponse finale.
    
    # 1. R√©cup√©ration explicite des docs (pour pouvoir logger ou yield si besoin)
    docs = retriever.invoke(standalone_question)
    formatted_context = format_docs(docs)
    
    yield {"type": "status", "content": "R√©daction de la r√©ponse..."}

    # 2. Cha√Æne de g√©n√©ration finale
    final_chain = prompt | llm | StrOutputParser()
    
    response_stream = final_chain.stream({
        "context": formatted_context,
        "chat_history": format_history(chat_history),
        "question": question,
        "soil_type": soil_type,
        "location": location,
        "introduction_instruction": intro_text
    })
    
    for chunk in response_stream:
        yield {"type": "chunk", "content": chunk}

if __name__ == "__main__":
    # Quick test
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--test", action="store_true")
    args = parser.parse_args()
    
    if args.test:
        print("Test de la cha√Æne RAG...")
        # Since it's a generator now, we iterate
        for event in query_rag("Quel est le meilleur moment pour semer le mil au S√©n√©gal ?"):
            if event["type"] == "chunk":
                print(event["content"], end="", flush=True)
            elif event["type"] == "status":
                print(f"\n[STATUS: {event['content']}]\n")
        print("\n\nFin du test.")
